{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques\n",
    "\n",
    "ensenble techniques - ml methods that combine predictions from several machine learning models and give one prediction; can belong to a family of models, all decision trees, or logictic regression etc. \n",
    "\n",
    "similar to having a committee of experts who agree on decision; central idea - committee is better than single expert. \n",
    "\n",
    "important to have different types of models, independent of one another, so that models and predictions are not identical. \n",
    "\n",
    "weak learners - only slightly better than random, but very simply models\n",
    "\n",
    "takeaways: \n",
    "1. models should be independent\n",
    "2. okay to have weak learners\n",
    "\n",
    "contructing:\n",
    "1. parrallel\n",
    "2. sequential\n",
    "\n",
    "bagging vs boosting (seq)\n",
    "\n",
    "less variance with more models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "unstable learning techniques - slight change in data causes significant shift in decision tree. \n",
    "\n",
    "random forest randomly selects observations/rows and specific features to build multiple decision trees and then acerage the results across all the trees. \n",
    "\n",
    "Eacha new trainig data sets picks a sample of observations with replacement from original data set. by sampling with replacement, some observations may be repeated in each new training dat set. \n",
    "\n",
    "stability could increase with multiple trees. \n",
    "\n",
    "classification - modes/voting\n",
    "\n",
    "regression - means/avg\n",
    "\n",
    "ensemble learning - using multiple models\n",
    "\n",
    "if given the same data, cart trees will grow identically. different data must be used...\n",
    "\n",
    "bootstrap aggregating (bagging) - common ml technique, sample the data, use different samples of data, use those samples to train different trees, sampling with replacement. \n",
    "\n",
    "ex: M=10\n",
    "m=M - high tree correlation, all trees are very similar\n",
    "m=2 - weak trees, weak ability to predict\n",
    "\n",
    "one tree is sensitive to data, however, many trees become robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "regarding overfitting, one model could be overfitting, but it is not overfitting on all the data, only the sample used to create the model\n",
    "\n",
    "bagging - bootstrap aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "in subsequent models, higher weight is given to data points that were incorrectly predicted in previous models. \n",
    "\n",
    "2 most popular techniques for boosting\n",
    "1. adaboosting (adaptive boosting) - over/under weighting data based on previous models. \n",
    "2. gradient boosting - changing data, getting residuals from previous data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On\n",
    "\n",
    "bagging classifiers benefit from complex models\n",
    "\n",
    "boosting classifiers benefit from simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
